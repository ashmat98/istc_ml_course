import numpy as np

def sgd_momentum(x, dx, config, state):
    """
        This is a very ugly implementation of sgd with momentum 
        just to show an example how to store old grad in state.
        Make this function faster if you can!
        
        Try to understand this code to be able to implement Adam,
        as the implementation is very similar

        HINT: examine the structures/shapes of x and dx
        
        x: list of lists
            - parameters
        dx: list of lists
            - gradients of parameters,
              same format as x above
        config: dict
            - momentum
            - learning_rate
        state: dict
            - old_grad
    """
    
    # x and dx have complex structure, old dx will be stored in a simpler one
    state.setdefault('old_grad', {})
    
    i = 0 
    for cur_layer_x, cur_layer_dx in zip(x,dx): 
        for cur_x, cur_dx in zip(cur_layer_x,cur_layer_dx):
            
            cur_old_grad = state['old_grad'].setdefault(i, np.zeros_like(cur_dx))
            
            cur_old_grad = config['momentum'] * cur_old_grad + \
                config['learning_rate'] * cur_dx
            
            state['old_grad'][i] = cur_old_grad
            
            if cur_old_grad.shape[0] == 1:
                cur_x = cur_x.reshape(cur_old_grad.shape)
            
            np.add(cur_x, -cur_old_grad, out=cur_x)
            i += 1     